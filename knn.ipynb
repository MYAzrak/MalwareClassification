{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, RocCurveDisplay, auc\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "t_df = pd.read_csv(\"PreprocessedUnscaledTernaryClassification.csv\") # t_df == ternanry dataframe for the Ransomeware/Spyware/Trojan classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_df.value_counts(\"SubType\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, df, useScaler=False):\n",
    "        self.avg_train_accuracy, self.avg_test_accuracy = [], []\n",
    "        self.avg_train_f1_score, self.avg_test_f1_score = [], []\n",
    "        self.avg_train_precision, self.avg_test_precision = [], []\n",
    "        self.avg_train_recall, self.avg_test_recall = [], []\n",
    "        self.hyperparameter = []\n",
    "\n",
    "        self.class_names = ['Ransomware', 'Spyware', 'Trojan']\n",
    "\n",
    "        self.y = df['SubType'].to_numpy()\n",
    "        self.X = df.drop(columns=['SubType']).to_numpy()\n",
    "        if useScaler:\n",
    "            scaler = StandardScaler()\n",
    "            self.X = scaler.fit_transform(self.X)\n",
    "\n",
    "    def computeMetrics(self, y_true, y_pred):\n",
    "        report = classification_report(y_true, y_pred, output_dict=True)\n",
    "        accuracy = report['accuracy']\n",
    "        f1_score = report['weighted avg']['f1-score']\n",
    "        precision = report['weighted avg']['precision']\n",
    "        recall = report['weighted avg']['recall']\n",
    "        return accuracy, f1_score, precision, recall\n",
    "    def getAvgScores(self, n=11):\n",
    "        for i in range(1, n):\n",
    "            temp_train_accuracy, temp_test_accuracy = [], []\n",
    "            temp_train_f1_score, temp_test_f1_score = [], []\n",
    "            temp_train_precision, temp_test_precision = [], []\n",
    "            temp_train_recall, temp_test_recall = [], []\n",
    "            \n",
    "            for _ in range(10):\n",
    "                X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size=0.2)\n",
    "                \n",
    "                knn = KNeighborsClassifier(n_neighbors=i)\n",
    "                knn.fit(X_train, y_train)\n",
    "                \n",
    "                # Training set metrics\n",
    "                train_pred = knn.predict(X_train)\n",
    "                test_pred = knn.predict(X_test)\n",
    "\n",
    "                train_acc, train_f1, train_precision, train_recall = self.computeMetrics(y_train, train_pred)\n",
    "                test_acc, test_f1, test_precision, test_recall = self.computeMetrics(y_test, test_pred)\n",
    "\n",
    "                temp_train_accuracy.append(train_acc)\n",
    "                temp_train_f1_score.append(train_f1)\n",
    "                temp_train_precision.append(train_precision)\n",
    "                temp_train_recall.append(train_recall)\n",
    "                \n",
    "                # Testing set metrics\n",
    "                temp_test_accuracy.append(test_acc)\n",
    "                temp_test_f1_score.append(test_f1)\n",
    "                temp_test_precision.append(test_precision)\n",
    "                temp_test_recall.append(test_recall)\n",
    "\n",
    "            # Compute and store average metrics for each hyperparameter value\n",
    "            self.avg_train_accuracy.append(sum(temp_train_accuracy) / 10)\n",
    "            self.avg_test_accuracy.append(sum(temp_test_accuracy) / 10)\n",
    "            self.avg_train_f1_score.append(sum(temp_train_f1_score) / 10)\n",
    "            self.avg_test_f1_score.append(sum(temp_test_f1_score) / 10)\n",
    "            self.avg_train_precision.append(sum(temp_train_precision) / 10)\n",
    "            self.avg_test_precision.append(sum(temp_test_precision) / 10)\n",
    "            self.avg_train_recall.append(sum(temp_train_recall) / 10)\n",
    "            self.avg_test_recall.append(sum(temp_test_recall) / 10)\n",
    "            self.hyperparameter.append(i)\n",
    "\n",
    "            # Print average metrics for the current hyperparameter value\n",
    "            print(\"\\nFor n_neighbors =\", i)\n",
    "            print(f\"Average accuracy (training: {self.avg_train_accuracy[-1]}, testing: {self.avg_test_accuracy[-1]})\")\n",
    "            print(f\"Average F1-score (training: {self.avg_train_f1_score[-1]}, testing: {self.avg_test_f1_score[-1]})\")\n",
    "            print(f\"Average Precision (training: {self.avg_train_precision[-1]}, testing: {self.avg_test_precision[-1]})\")\n",
    "            print(f\"Average Recall (training: {self.avg_train_recall[-1]}, testing: {self.avg_test_recall[-1]})\")\n",
    "\n",
    "    def plotAccuracy(self):\n",
    "        plt.figure(figsize=(15, 6))\n",
    "\n",
    "        # Plot training and testing accuracy\n",
    "        plt.plot(self.hyperparameter, self.avg_train_accuracy, label='Training Accuracy', marker='o', linestyle='-')\n",
    "        plt.plot(self.hyperparameter, self.avg_test_accuracy, label='Testing Accuracy', marker='o', linestyle='-')\n",
    "\n",
    "        plt.title('Accuracy vs. n_neighbors')\n",
    "        plt.xlabel('n_neighbors')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.xticks(self.hyperparameter)\n",
    "        plt.show()\n",
    "\n",
    "    def trainTestSplit(self):\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=0.2)\n",
    "\n",
    "    def plotConfusionMatrix(self, n_neigh):\n",
    "\n",
    "        if not hasattr(self, 'X_train'):\n",
    "            print(\"Please call trainTestSplit() first\")\n",
    "            return\n",
    "\n",
    "        knn = KNeighborsClassifier(n_neighbors=n_neigh)\n",
    "        knn.fit(self.X_train, self.y_train)\n",
    "\n",
    "        y_train_pred = knn.predict(self.X_train)\n",
    "        y_test_pred = knn.predict(self.X_test)\n",
    "\n",
    "        # Training Confusion matrix\n",
    "        cm = confusion_matrix(self.y_train, y_train_pred)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        cm_display = ConfusionMatrixDisplay(cm, display_labels=self.class_names).plot(cmap='Blues', values_format='d')\n",
    "        plt.title('Confusion Matrix (Training)')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.show()\n",
    "\n",
    "        # Testing Confusion matrix\n",
    "        cm = confusion_matrix(self.y_test, y_test_pred)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        cm_display = ConfusionMatrixDisplay(cm, display_labels=self.class_names).plot(cmap='Blues', values_format='d')\n",
    "        plt.title('Confusion Matrix (Testing)')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.show()\n",
    "\n",
    "    def plotROCCurve(self, n_neigh):\n",
    "        if not hasattr(self, 'X_train'):\n",
    "            print(\"Please call trainTestSplit() first\")\n",
    "            return\n",
    "\n",
    "        knn = KNeighborsClassifier(n_neighbors=n_neigh)\n",
    "\n",
    "        # Initialize the One-vs-Rest classifier\n",
    "        ovr_classifier = OneVsRestClassifier(knn)\n",
    "\n",
    "        # Train the classifier and obtain the predicted probabilities for each class\n",
    "        y_train_score = ovr_classifier.fit(self.X_train, self.y_train).predict_proba(self.X_train)\n",
    "        y_test_score = ovr_classifier.predict_proba(self.X_test)\n",
    "\n",
    "        # Compute ROC curve and ROC area for each class on training data\n",
    "        fpr_train = dict()\n",
    "        tpr_train = dict()\n",
    "        roc_auc_train = dict()\n",
    "        for i in range(len(ovr_classifier.classes_)):\n",
    "            y_train_onevsall = np.array([1 if label == ovr_classifier.classes_[i] else 0 for label in self.y_train])\n",
    "            fpr_train[i], tpr_train[i], _ = roc_curve(y_train_onevsall, y_train_score[:, i])\n",
    "            roc_auc_train[i] = auc(fpr_train[i], tpr_train[i])\n",
    "\n",
    "        # Compute ROC curve and ROC area for each class on test data\n",
    "        fpr_test = dict()\n",
    "        tpr_test = dict()\n",
    "        roc_auc_test = dict()\n",
    "        for i in range(len(ovr_classifier.classes_)):\n",
    "            y_test_onevsall = np.array([1 if label == ovr_classifier.classes_[i] else 0 for label in self.y_test])\n",
    "            fpr_test[i], tpr_test[i], _ = roc_curve(y_test_onevsall, y_test_score[:, i])\n",
    "            roc_auc_test[i] = auc(fpr_test[i], tpr_test[i])\n",
    "\n",
    "        # Create a new figure and axis object\n",
    "        plt.figure(figsize=(8, 6))\n",
    "\n",
    "        # Loop through each class\n",
    "        for i in range(len(ovr_classifier.classes_)):\n",
    "            # Plot the ROC curve for training data on the same axis\n",
    "            roc_display_train = RocCurveDisplay(fpr=fpr_train[i], tpr=tpr_train[i], roc_auc=roc_auc_train[i])\n",
    "            roc_display_train.plot(ax=plt.gca(), name=f'Train (Class {self.class_names[i]})', linestyle='--')\n",
    "\n",
    "            # Plot the ROC curve for test data\n",
    "            roc_display_test = RocCurveDisplay(fpr=fpr_test[i], tpr=tpr_test[i], roc_auc=roc_auc_test[i])\n",
    "            roc_display_test.plot(ax=plt.gca(), name=f'Test (Class {self.class_names[i]})')\n",
    "\n",
    "        # Set labels and title for the plot\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curves for All Classes')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results without feature selection and dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNN(t_df, useScaler=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.getAvgScores(n=9001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.plotAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.trainTestSplit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.plotConfusionMatrix(1) # Best n_neighbors is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.plotROCCurve(1) # Best n_neighbors is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results with feature selection and dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df = pd.read_csv('PCA_Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNN(pca_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.getAvgScores(9001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.plotAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.trainTestSplit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.plotConfusionMatrix(3) # Best n_neighbors is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.plotROCCurve(5) # Best n_neighbors is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
